# 実験名未定

## plan by nishio

(nishio) logが7件集まったので、人間のプレイを教師データとして学習したい。どのような方法が良いと思うか？

### 目的の確認（最初に決める）

- 人間の意思決定を**再現**するのが目的か（模倣精度重視）
- それとも**勝率を上げる**のが目的か（人間らしさより強さ）

ここでは「人間のプレイを教師データとして学習」に合わせて、**模倣精度を主目標**にしたプランを提案する。

→nishio: 勝率を上げることが目的である、現状の人間もあまり強くはない。
  - 1: 人間から学習したlr1を作る
  - 2: lr1がheiristic2に対して強いかどうかを自己対戦で実験する
  - 3: ログの「勝者/敗者」から学習したlr2を作る（敗者は選択を反転）
  - 4: これがlr1より強いかどうかを自己対戦で確認する
  - 前提: 人間はheuristic2より強い

### 推奨アプローチ

データが7ログと少ないので、複雑な学習よりも**解釈可能で低次元**の手法が良い。

1. **HeuristicParamsの同定（パラメータ学習）**  
   既存の `HeuristicAI` をベースに、閾値・milking・blockingを人間の選択に近づける。
2. **軽量な行動クローン（分類）**  
   1手ごとの TAKE/PASS を教師にしてロジスティック回帰 or 決定木で学習。

まずは1で作業量を抑えつつ、2で「人間が重視している特徴」が見えるようにするのが良い。

(nishio) 同感です。AIプレイヤーの行動のバリエーションを増やすことを考えて、ロジスティック回帰してpredict_probaして確率的に行動決定することを考えている。

### 具体プラン

1. **ログ解析（教師データ化）**
   - 各ログから「人間の意思決定点」を抽出
   - 状態特徴（入力）を作成
     - `Δscore`（take_cost）, tokens_on_card, deck_left, 自分/相手のtokens
     - runの両端にある未公開カード数（自分/相手）　# nishio: これは低コストカードの出現確率と関連している、デッキの残りカード数を掛けたものも追加で特徴量にすべき
     - 相対スコア（自分−最小相手）
     - (追加特徴案 by nishio):
       - 自分がpassして他人が取った時に発生するチップ格差を恐れる気持ち(特に残りデッキ数が多い時に)を表現したい
         - またその時に自分がチップ数でビリになるかどうか
         - チップ数でビリになる人のチップ数
         - チップ数でトップとビリの差
       - トップとの点差
         - 残りデッキで割ったもの
       - 自分が1枚以上カードを取っているか
       - HeuristicParamsのparamが掛け算されている対象
         - HeuristicがLRで再現できるように特徴量に分解すると良い
   - 教師ラベルは TAKE/PASS

2. **ベースライン評価**
   - 既存 heuristic の人間一致率（accuracy）を測る
   - 混同行列（TAKEすべき時にPASSしがち、など）を確認

4. **軽量分類モデル（補助）**
   - ロジスティック回帰
   - 重みの観察

5. **評価**
  - 1: 人間から学習したlr1を作る
  - 2: lr1がheiristic2に対して強いかどうかを自己対戦で実験する
  - 3: ログの「勝者/敗者」から学習したlr2を作る（敗者は選択を反転）
  - 4: これがlr1より強いかどうかを自己対戦で確認する


### 成果物

- `logs` から教師データを生成するスクリプト
- `lr1`, `lr2`（人間に近いパラメータ）
- 実験レポート（一致率・比較結果）

---

## 実行結果

### lr1（人間ログ）

```
python scripts/train_lr.py --out models/lr1.json
```

- logs=7 / samples=288 / take率=13.89%
- heuristic一致率=85.07% / heuristic2一致率=88.54%
- lr1の学習一致率=76.74%

### lr1 vs heuristic2（自己対戦）

```
python nothanks_cui.py simulate --games 3000 --seed 1 --ai lr1 heuristic2 heuristic2
python nothanks_cui.py simulate --games 3000 --seed 1 --ai heuristic2 lr1 heuristic2
python nothanks_cui.py simulate --games 3000 --seed 1 --ai heuristic2 heuristic2 lr1
```

- lr1 winrate: 24.43% / 24.82% / 23.38%（平均 24.21%）
- 結論: 現状のlr1はheuristic2より弱い

### lr2（勝ちログ + 負けログ反転）

```
python scripts/train_lr.py --out models/lr2.json --invert-losses
```

- logs=7 / samples=288 / take率=62.85%
- lr2の学習一致率=75.35%

### lr2 vs lr1（自己対戦）

```
python nothanks_cui.py simulate --games 3000 --seed 1 --ai lr2 lr1 lr1
python nothanks_cui.py simulate --games 3000 --seed 1 --ai lr1 lr2 lr1
python nothanks_cui.py simulate --games 3000 --seed 1 --ai lr1 lr1 lr2
```

- lr2 winrate: 0.07% / 0.00% / 0.00%（平均 0.02%）
- 結論: 反転データは過激すぎて崩壊。lr2はlr1より大幅に弱い

### lr2 vs heuristic2（自己対戦）

```
python nothanks_cui.py simulate --games 3000 --seed 1 --ai lr2 heuristic2 heuristic2
```

- lr2 winrate: 0.23%
- 結論: heuristic2に完敗

---

## Method A（自己対戦での直接最適化）Plan

目的: **勝率最大化**を目的関数として、ロジスティック回帰の重みを直接最適化する。

手法:

- ベースモデル: `models/lr1.json`
- 最適化: ランダム探索（ガウスノイズ付きのhill-climb）
  - 初期解はlr1の重み/バイアス
  - 各反復で `weight += N(0, sigma)` / `bias += N(0, sigma_bias)` した候補を作る
  - 候補を自己対戦で評価し、**勝率が上がる**（同率なら平均スコアが改善）場合のみ採用
    - 1候補の評価は `games_per_seat`×3座席。今回の設定は `games_per_seat=200` なので 600戦
  - これを `iters` 回繰り返す（局所探索・勾配なし）。今回の実行は `iters=80` なので更新ステップは80回
- 評価: `heuristic2`×2 との自己対戦を **3座席ローテ**
- 目的関数: winrate（同率なら平均スコアでtie-break）

実行手順:

1. `scripts/optimize_lr.py` を作成
2. `python scripts/optimize_lr.py --base models/lr1.json --out models/lr_opt.json`
3. 最良モデルの勝率/平均スコアを記録

## Method A（自己対戦での直接最適化）結果

```
python scripts/optimize_lr.py --base models/lr1.json --out models/lr_opt.json
```

- BASE winrate=23.67% / mean_score=73.04
- BEST winrate=68.00% / mean_score=45.56
- params: games_per_seat=200, iters=80, sigma=0.15, eval_seed=1
- note: 探索と評価が同じseedなので、別seedでの検証が必要

### Method A（別seedでの検証）

```
python scripts/optimize_lr.py --base models/lr_opt.json --out models/lr_opt_eval_seed2.json --iters 0 --eval-seed 2
```

- winrate=62.67% / mean_score=47.78

```
% python ./nothanks_cui.py simulate --games 1000 --seed 2 --ai lr_opt heuristic2 heuristic2

SIMULATION RESULTS
games=1000 seed=2 AIs=('lr_opt', 'heuristic2', 'heuristic2')
------------------------------------------------------------
AI0 (lr_opt   )  mean= 45.88  sd= 17.09  winrate=67.60%
AI1 (heuristic2)  mean= 69.46  sd= 24.79  winrate=18.60%
AI2 (heuristic2)  mean= 75.77  sd= 25.63  winrate=13.80%
------------------------------------------------------------
```

### 人間の体感

僕より強いw

## LR特徴量の算出と影響（lr_opt）

- `models/lr_opt.json` の重み。特徴量は標準化されるため、`z = (x - mean) / std` を使う
- `score` は低いほど良い
- `unknown` は 3..35 から「全プレイヤーのカード + active」を除いた集合（除外9枚も未公開扱いで含まれる）

| 特徴量 | 算出方法 | weight | 判断への影響 |
| --- | --- | --- | --- |
| take_cost | take_delta_cardpts - tokens_on_card（TAKE即時Δscore） | -2.0787 | コストが大きいほどPASS寄り |
| take_delta_cardpts | marginal_card_points(自分のカード, active) | -1.0942 | カード点の増分が大きいほどPASS寄り |
| tokens_on_card | 場に乗っているチップ数 | +1.1796 | 多いほどTAKE寄り |
| tokens_on_card_plus1 | tokens_on_card + 1（PASS後に+1される分） | +1.4191 | 多いほどTAKE寄り |
| deck_left | 残り山札枚数 | +0.9365 | 多いほどTAKE寄り |
| my_tokens | 自分のチップ数 | -0.4249 | 多いほどPASS寄り |
| min_opp_tokens | 相手チップの最小値 | -0.0408 | 相手が多いほどPASS寄り（効果小） |
| max_opp_tokens | 相手チップの最大値 | +0.0262 | 相手が多いほどTAKE寄り（効果小） |
| mean_opp_tokens | 相手チップの平均 | +0.2411 | 相手が多いほどTAKE寄り |
| token_gap | 全員のチップ差（最大-最小） | +0.0081 | 差が大きいほどTAKE寄り（効果小） |
| min_tokens_after_pass | PASS後のチップ最小値（自分だけ-1した後の全員の最小チップ数） | -1.1455 | 高いほどPASS寄り |
| max_tokens_after_pass | PASS後のチップ最大値（自分の-1反映） | -0.6200 | 高いほどPASS寄り |
| token_gap_after_pass | PASS後のチップ差（最大-最小） | +0.8460 | 差が大きいほどTAKE寄り |
| would_be_min_tokens | PASS後に自分が最小チップなら1 | -0.7366 | 1だとPASS寄り |
| my_score | 自分の暫定スコア（低いほど良い） | -0.9396 | 高いほどPASS寄り |
| min_opp_score | 相手の最小スコア（リーダー） | +0.1025 | 相手が悪いほどTAKE寄り |
| score_gap_to_leader | my_score - min_opp_score（正なら遅れている） | +1.0134 | 遅れているほどTAKE寄り |
| score_gap_to_leader_norm | score_gap_to_leader / max(deck_left,1) | +0.0717 | 遅れているほどTAKE寄り（効果小） |
| has_cards | 自分が1枚以上持っていれば1 | -0.4250 | 1だとPASS寄り |
| edges_unknown_self | 自分の連番の両端にある未公開カード数 | +0.0999 | 多いほどTAKE寄り（効果小） |
| edges_unknown_self_mul_deck | edges_unknown_self * deck_left | -0.5402 | 多いほどPASS寄り |
| edges_unknown_opp_min | 相手ごとに「連番の端に隣接する未公開カード数」を数え、その最小値 | +1.0154 | 相手が多いほどTAKE寄り |
| edges_unknown_opp_max | 相手のedges_unknownの最大値 | +0.6099 | 相手が多いほどTAKE寄り |
| edges_unknown_opp_mean | 相手のedges_unknownの平均 | -0.8011 | 相手が多いほどPASS寄り |
| lowtoken_term | 1/(my_tokens+1) | +0.4377 | チップが少ないほどTAKE寄り |
| endgame_frac | 1 - deck_left/23 | +0.0480 | 終盤ほどTAKE寄り（効果小） |

※ 相関した特徴量が多く、単独の重みだけで挙動が決まるわけではない点に注意。

### 実践プレイ向けの読み替え（単位あたりの目安）

LRは内部で「スコア = bias + Σ(weight * z)」を作り、スコアが高いほどTAKE寄りになる。  
生の値を +1 したときの**スコア変化**は `weight/std` で近似できる（lr_optの学習分布に基づく目安）。

- 残り山札 +1枚 → スコア +0.13
- 場のチップ +1枚 → +0.20
- TAKE即時コスト +1点 → -0.19
- カード点の増分 +1点 → -0.10
- 自分チップ +1枚 → -0.05
- PASS後に自分だけ-1した状態での全員のチップ最小値 +1枚 → -0.41（強いPASS寄り）
- PASS後のチップ格差 +1 → +0.09
- リーダーとの差 +1点 → +0.045
- 相手ごとに“連番の端に隣接する未公開カード数”を数えて、その最小値 +1枚 → +0.60（強いTAKE寄り）
- 低チップの影響（lowtoken_term）:
  - 0→1枚でスコア -2.81
  - 1→2枚で -0.94
  - 2→3枚で -0.47
